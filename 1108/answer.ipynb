{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3回 (11/16) Chainerを使ってDeep Learning手法を実装しよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 前回のMLPをChainerで書いてみよう\n",
    "- Chainer:  日本のPreffered Networksという企業が開発したDeep Learning用ライブラリ。\n",
    "\n",
    "\n",
    "- 前回のように、順伝播、誤差逆伝播、パラメータ更新のそれぞれをプログラムする必要はない。\n",
    "- 必要なのは、ネットワーク構造の定義と、順伝播(=入力x, tからlossを計算する過程)の定義のみ。\n",
    "- 面倒なことはchainer.optimizerというやつがたったの1行でやってくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Variable, Chain, optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTデータの読み込み\n",
    "- 前回と違ってラベルはone-of-k表現にしなくてOK。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "dir = '/data/ishimochi0/dataset/mnist/'\n",
    "\n",
    "X_train = np.loadtxt(dir + 'train-images.txt').astype(np.float32).reshape((-1, 784)) / 255.\n",
    "y_train = np.loadtxt(dir + 'train-labels.txt').astype(np.int32)\n",
    "N_train = len(X_train)\n",
    "\n",
    "X_test= np.loadtxt(dir + 'test-images.txt').astype(np.float32).reshape((-1, 784)) / 255.\n",
    "y_test = np.loadtxt(dir + 'test-labels.txt').astype(np.int32)\n",
    "N_test = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPクラス\n",
    "- init関数 = ネットワーク構造の定義\n",
    " - 線形層はL.Linear(in_dim, out_dim)で定義する。\n",
    " - 活性化関数のことはここでは記述しない。更新するパラメータを持ったものだけを書く。\n",
    " \n",
    " \n",
    "- call関数 = 順伝播の定義\n",
    " - 活性化関数はF.sigmoid, F.reluのように呼び出す。\n",
    " - F.softmax_cross_entropyがsoftmaxとlossの計算をセットでやってくれるので、self.yはsoftmaxを掛ける前の値であることに注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.fc1 = L.Linear(784, 1000)\n",
    "            self.fc2 = L.Linear(1000, 1000)\n",
    "            self.fc3 = L.Linear(1000, 10)\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        h = F.sigmoid(self.fc1(x))\n",
    "        h = F.sigmoid(self.fc2(h))\n",
    "        self.y = self.fc3(h)\n",
    "        self.loss = F.softmax_cross_entropy(self.y, t)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義とoptimizerの設定\n",
    "- 前回実装した最適化手法はパラメータを勾配方向に更新するだけのシンプルな手法で、確率的勾配降下法(Stochastic Gradient Decent: SGD)と呼ばれる。\n",
    "- 前回の実装ではlr=0.01としたが、これは今回の実装ではlr=1.0に相当する(batchsize=100の場合)。前回の実装ではミニバッチ内の全データの勾配の「合計」を用いてパラメータを更新していたが、Chainerでは「平均」を用いるので、同じlrではパラメータの更新量が小さくなる。というか本来はそうするべき？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MLP() # モデル作成\n",
    "model.to_gpu() # GPUに載せる\n",
    "optimizer = optimizers.SGD(lr=1.0)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTの学習\n",
    "- 前回とほとんど同じです！ただし注意点。\n",
    " - chainerでは、入力変数は\"Variable型\"というやつに変形しておく必要がある。\n",
    " - そして、入力に対して順伝播が計算されていくが、その過程で生じる変数も全部Variable型。出力yや、lossもVariable型。\n",
    " - Variable型のデータから中身(=numpy array)を取り出すには、\".data\"とする。\n",
    "\n",
    "例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([ 5.])\n",
      "[ 5.]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([5], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "\n",
    "print(x) # これだと意味不明なアドレスが表示される\n",
    "print(x.data) # 中身の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train loss 0.0377430974972 | accuracy 0.988666666667\n",
      "Test loss 0.0736006769713 | accuracy 0.9761\n",
      "epoch 1\n",
      "Train loss 0.034131549179 | accuracy 0.98965\n",
      "Test loss 0.066456923708 | accuracy 0.9785\n",
      "epoch 2\n",
      "Train loss 0.0309158712478 | accuracy 0.991066666667\n",
      "Test loss 0.0662971557525 | accuracy 0.9796\n",
      "epoch 3\n",
      "Train loss 0.0277806539985 | accuracy 0.992133333333\n",
      "Test loss 0.0707003810546 | accuracy 0.9787\n",
      "epoch 4\n",
      "Train loss 0.0253880935822 | accuracy 0.993116666667\n",
      "Test loss 0.0683887335024 | accuracy 0.9784\n",
      "epoch 5\n",
      "Train loss 0.0234229001391 | accuracy 0.99355\n",
      "Test loss 0.0681526364241 | accuracy 0.9794\n",
      "epoch 6\n",
      "Train loss 0.021178430815 | accuracy 0.994366666667\n",
      "Test loss 0.064279399443 | accuracy 0.9799\n",
      "epoch 7\n",
      "Train loss 0.0187197637801 | accuracy 0.995433333333\n",
      "Test loss 0.0639767580499 | accuracy 0.9813\n",
      "epoch 8\n",
      "Train loss 0.0174248694595 | accuracy 0.995466666667\n",
      "Test loss 0.0632141828359 | accuracy 0.9812\n",
      "epoch 9\n",
      "Train loss 0.0156684828868 | accuracy 0.99625\n",
      "Test loss 0.0662767021573 | accuracy 0.9797\n",
      "epoch 10\n",
      "Train loss 0.0144193413272 | accuracy 0.9969\n",
      "Test loss 0.0654485653545 | accuracy 0.9802\n",
      "epoch 11\n",
      "Train loss 0.0130069568291 | accuracy 0.997066666667\n",
      "Test loss 0.0644014407873 | accuracy 0.9809\n",
      "epoch 12\n",
      "Train loss 0.0118374053277 | accuracy 0.997566666667\n",
      "Test loss 0.0666814139715 | accuracy 0.9806\n",
      "epoch 13\n",
      "Train loss 0.0104847803175 | accuracy 0.997883333333\n",
      "Test loss 0.0629455776158 | accuracy 0.9814\n",
      "epoch 14\n",
      "Train loss 0.00964024217062 | accuracy 0.998366666667\n",
      "Test loss 0.0661014093681 | accuracy 0.9811\n",
      "epoch 15\n",
      "Train loss 0.00891364944478 | accuracy 0.998416666667\n",
      "Test loss 0.0638680752873 | accuracy 0.9817\n",
      "epoch 16\n",
      "Train loss 0.00777860440052 | accuracy 0.9989\n",
      "Test loss 0.0680204779071 | accuracy 0.9798\n",
      "epoch 17\n",
      "Train loss 0.00725212645028 | accuracy 0.998966666667\n",
      "Test loss 0.0635779508504 | accuracy 0.9815\n",
      "epoch 18\n",
      "Train loss 0.00648800615929 | accuracy 0.99915\n",
      "Test loss 0.0640068952332 | accuracy 0.9822\n",
      "epoch 19\n",
      "Train loss 0.00597429470028 | accuracy 0.999333333333\n",
      "Test loss 0.0640290107606 | accuracy 0.9817\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print('epoch', epoch)\n",
    "    \n",
    "    # Training\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    perm = np.random.permutation(N_train)\n",
    "    \n",
    "    for i in range(0, N_train, batchsize):\n",
    "        # ミニバッチの作成。GPUに送った後、Variable型に変える\n",
    "        x = Variable(cuda.to_gpu(X_train[perm[i: i+batchsize]]))\n",
    "        t = Variable(cuda.to_gpu(y_train[perm[i: i+batchsize]]))\n",
    "        \n",
    "        optimizer.update(model, x, t) # 順伝播、誤差逆伝播、パラメータの更新\n",
    "        sum_loss += cuda.to_cpu(model.loss.data) * len(x.data) # Variable型の中身を取得して、CPUに戻す (戻さなくても大丈夫なこともある)\n",
    "        pred_y.extend(np.argmax(cuda.to_cpu(model.y.data), axis=1)) # yは前回と違ってsoftmax掛ける前だけど問題ない\n",
    "        # ミニバッチ毎の識別率はF.accuracy(y, t)でも計算できるけどカウンタの設置が必要だし応用が利かないので却下\n",
    "        \n",
    "    loss = sum_loss / N_train\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * np.eye(10)[y_train[perm]]) / N_train\n",
    "    print('Train loss', loss, '| accuracy', accuracy)  \n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    \n",
    "    for i in range(0, N_test, batchsize):\n",
    "        x = Variable(cuda.to_gpu(X_test[i: i+batchsize]))\n",
    "        t = Variable(cuda.to_gpu(y_test[i: i+batchsize]))\n",
    "        \n",
    "        sum_loss += cuda.to_cpu(model(x, t).data) * len(x.data) # 順伝播\n",
    "        pred_y.extend(np.argmax(cuda.to_cpu(model.y.data), axis=1))\n",
    "\n",
    "    loss = sum_loss / N_test\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * np.eye(10)[y_test]) / N_test\n",
    "    print('Test loss', loss, '| accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: MNISTをCNNで識別してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNとは\n",
    "- 畳み込み層とプーリング層\n",
    " - 畳み込み層が色々な特徴を抽出し、プーリング層がそれを集約する。\n",
    "<img src=\"figure/cnn.JPG\", width=800>\n",
    "\n",
    "\n",
    "- 畳み込みとプーリングを繰り返したら、程よいところでベクトルに引き伸ばして、全結合層(=線形層)を繋げて識別する。\n",
    " - 全結合層は英語で書くとfully-connected layerで、よく「fc層」と呼んだりする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTデータの読み込み\n",
    "- CNNを使うときは、データXはデータ数\\*カラーチャンネル数\\*縦\\*横の4次元テンソルの形にする。\n",
    "- 混乱するとアレなのでもう一回読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "dir = '/data/ishimochi0/dataset/mnist/'\n",
    "\n",
    "X_train = np.loadtxt(dir + 'train-images.txt').astype(np.float32).reshape((-1, 1, 28, 28)) / 255.\n",
    "y_train = np.loadtxt(dir + 'train-labels.txt').astype(np.int32)\n",
    "N_train = len(X_train)\n",
    "\n",
    "X_test= np.loadtxt(dir + 'test-images.txt').astype(np.float32).reshape((-1, 1, 28, 28)) / 255.\n",
    "y_test = np.loadtxt(dir + 'test-labels.txt').astype(np.int32)\n",
    "N_test = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNクラス\n",
    "- 畳み込み層はL.Convolution2D(入力チャンネル数、出力チャンネル数、フィルタサイズ)で定義する。\n",
    "- その他の引数:\n",
    " - stride: デフォルトは1。基本的に弄らなくてよい。\n",
    " - pad: デフォルトは0。初回の演習でやったように、出力サイズは入力サイズよりも小さくなるので、あまりDeeeepにはできない。それが嫌なら、padを指定しよう。 \n",
    "\n",
    "\n",
    "- プーリングはF.max_pooling_2d(x, size)のように呼び出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CNN(Chain):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(1, 20, 5)\n",
    "            self.conv2 = L.Convolution2D(20, 50, 5)\n",
    "            self.fc3 = L.Linear(800, 500)\n",
    "            self.fc4 = L.Linear(500, 10)\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "    \n",
    "        h = F.relu(self.fc3(h))\n",
    "        self.y = self.fc4(h)\n",
    "        self.loss = F.softmax_cross_entropy(self.y, t)\n",
    "        \n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義とoptimizerの設定\n",
    "- 今回は最適化手法にMomemtumSGDを使う。\n",
    " - 現在の勾配に、過去の勾配をある割合(0.9が一般的)で加えたものを使ってパラメータを更新。\n",
    " - SGDに比べて、振動が抑えられ、安定して学習が進むようになる。\n",
    "- Weight Decayも導入してみる。\n",
    " - 重みの値が大きくなることに対して罰則を加える -> 過学習を防げる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CNN() \n",
    "model.to_gpu()\n",
    "optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(0.0005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNの学習\n",
    "- さっきと全く同じです！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train loss 0.210450467332 | accuracy 0.937416666667\n",
      "Test loss  0.0608491955325 | accuracy  0.9804\n",
      "epoch 1\n",
      "Train loss 0.0624794593376 | accuracy 0.98025\n",
      "Test loss  0.0453843436053 | accuracy  0.9856\n",
      "epoch 2\n",
      "Train loss 0.0454200434165 | accuracy 0.985666666667\n",
      "Test loss  0.0498993334919 | accuracy  0.9841\n",
      "epoch 3\n",
      "Train loss 0.035279705843 | accuracy 0.989016666667\n",
      "Test loss  0.0318839562109 | accuracy  0.9901\n",
      "epoch 4\n",
      "Train loss 0.0289386086546 | accuracy 0.9913\n",
      "Test loss  0.035740500268 | accuracy  0.9883\n",
      "epoch 5\n",
      "Train loss 0.0250872532746 | accuracy 0.992283333333\n",
      "Test loss  0.0328098152308 | accuracy  0.9893\n",
      "epoch 6\n",
      "Train loss 0.0227422051321 | accuracy 0.993066666667\n",
      "Test loss  0.0351577963938 | accuracy  0.9885\n",
      "epoch 7\n",
      "Train loss 0.0192432374331 | accuracy 0.994233333333\n",
      "Test loss  0.0301677836797 | accuracy  0.9886\n",
      "epoch 8\n",
      "Train loss 0.0177193273854 | accuracy 0.99495\n",
      "Test loss  0.0293604841243 | accuracy  0.9897\n",
      "epoch 9\n",
      "Train loss 0.0162710440721 | accuracy 0.995016666667\n",
      "Test loss  0.0280099764742 | accuracy  0.99\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "batchsize = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print('epoch', epoch)\n",
    "    \n",
    "    # Training\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    perm = np.random.permutation(N_train)\n",
    "    \n",
    "    for i in range(0, N_train, batchsize):\n",
    "        x = Variable(cuda.to_gpu(X_train[perm[i: i+batchsize]]))\n",
    "        t = Variable(cuda.to_gpu(y_train[perm[i: i+batchsize]]))\n",
    "        \n",
    "        optimizer.update(model, x, t)\n",
    "        sum_loss += cuda.to_cpu(model.loss.data) * len(x.data)\n",
    "        pred_y.extend(np.argmax(cuda.to_cpu(model.y.data), axis=1)) \n",
    "    \n",
    "    loss = sum_loss / N_train\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * np.eye(10)[y_train[perm]]) / N_train\n",
    "    print('Train loss', loss, '| accuracy', accuracy)  \n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    \n",
    "    for i in range(0, N_test, batchsize):\n",
    "        x = Variable(cuda.to_gpu(X_test[i: i+batchsize]))\n",
    "        t = Variable(cuda.to_gpu(y_test[i: i+batchsize]))\n",
    "        \n",
    "        sum_loss += cuda.to_cpu(model(x, t).data) * len(x.data)\n",
    "        pred_y.extend(np.argmax(cuda.to_cpu(model.y.data), axis=1))\n",
    "\n",
    "    loss = sum_loss / N_test\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * np.eye(10)[y_test]) / N_test\n",
    "    print('Test loss ', loss, '| accuracy ', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3: CIFAR-10をCNNで識別してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10の読み込み\n",
    "- 飛行機、自動車、鳥、猫、鹿、犬、蛙、馬、船、トラックの10クラス\n",
    "- 訓練データ50000枚、テストデータ10000枚\n",
    "- 32\\*32のRGB画像\n",
    "- 研究室のサーバから読み込む\n",
    "\n",
    "<img src=\"figure/cifar-10.JPG\", width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fp:\n",
    "        if sys.version_info.major == 2:\n",
    "            data = pickle.load(fp)\n",
    "        elif sys.version_info.major == 3:\n",
    "            data = pickle.load(fp, encoding='latin-1')\n",
    "    return data\n",
    "\n",
    "train = [unpickle('/data/ishimochi0/dataset/cifar-10-batches-py/data_batch_%d' %i) for i in range(1,6)]\n",
    "X_train_ = np.concatenate([d['data'] for d in train]).reshape((-1, 3, 32, 32)).astype('float32') / 255.\n",
    "y_train = np.concatenate([d['labels'] for d in train]).astype('int32')\n",
    "N_train = len(X_train_)\n",
    "\n",
    "test = unpickle('/data/ishimochi0/dataset/cifar-10-batches-py/test_batch')\n",
    "X_test_ = test['data'].reshape((-1, 3, 32, 32)).astype('float32') / 255.\n",
    "y_test = np.array(test['labels'], dtype='int32')\n",
    "N_test = len(X_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理\n",
    "255で割るだけでなく、学習データの平均を引くと2〜3%くらい精度が上がります！\n",
    "- データの分布が0中心になる->ReLUの非線形性をフル活用できる\n",
    "- 各カラーチャンネルごとに平均を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(X_train_, X_test_):\n",
    "    X_mean = np.mean(X_train_, axis=(0, 2, 3), keepdims=True)\n",
    "    X_train = X_train_ - X_mean\n",
    "    X_test = X_test_ - X_mean\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNクラス\n",
    "- 自分でネットワークを構築してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CNN(Chain):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(3, 32, 5, pad=2)\n",
    "            self.conv2 = L.Convolution2D(32, 64, 5, pad=2)\n",
    "            self.conv3 = L.Convolution2D(64, 128, 5, pad=2)\n",
    "            self.fc4 = L.Linear(16*128, 128)\n",
    "            self.fc5 = L.Linear(128, 10)\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "    \n",
    "        h = F.relu(self.fc4(h))\n",
    "        self.y = self.fc5(h)\n",
    "        self.loss = F.softmax_cross_entropy(self.y, t)\n",
    "        \n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義とoptimizerの設定\n",
    "- さっきと同じです！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CNN() \n",
    "model.to_gpu()\n",
    "optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(0.0005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10の学習\n",
    "- データの前処理が加わりました！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train loss 1.58841254425 | accuracy 0.42392\n",
      "Test loss  1.27095008016 | accuracy  0.5339\n",
      "epoch 1\n",
      "Train loss 1.15325602698 | accuracy 0.58908\n",
      "Test loss  1.04236194789 | accuracy  0.6243\n",
      "epoch 2\n",
      "Train loss 0.940291112542 | accuracy 0.66822\n",
      "Test loss  0.900817964673 | accuracy  0.681\n",
      "epoch 3\n",
      "Train loss 0.786138306975 | accuracy 0.72376\n",
      "Test loss  0.87589012742 | accuracy  0.6987\n",
      "epoch 4\n",
      "Train loss 0.678492396355 | accuracy 0.76382\n",
      "Test loss  0.811496055722 | accuracy  0.7245\n",
      "epoch 5\n",
      "Train loss 0.586351692796 | accuracy 0.79342\n",
      "Test loss  0.775932582617 | accuracy  0.7366\n",
      "epoch 6\n",
      "Train loss 0.505986744761 | accuracy 0.82432\n",
      "Test loss  0.742292573452 | accuracy  0.751\n",
      "epoch 7\n",
      "Train loss 0.427829537928 | accuracy 0.8503\n",
      "Test loss  0.790488277674 | accuracy  0.7493\n",
      "epoch 8\n",
      "Train loss 0.361187461138 | accuracy 0.87356\n",
      "Test loss  0.829762749374 | accuracy  0.7341\n",
      "epoch 9\n",
      "Train loss 0.298232761681 | accuracy 0.895\n",
      "Test loss  0.834604891539 | accuracy  0.744\n",
      "epoch 10\n",
      "Train loss 0.240132400781 | accuracy 0.91528\n",
      "Test loss  0.893707203269 | accuracy  0.7416\n",
      "epoch 11\n",
      "Train loss 0.191580037057 | accuracy 0.93394\n",
      "Test loss  0.970017129779 | accuracy  0.7372\n",
      "epoch 12\n",
      "Train loss 0.160518251389 | accuracy 0.94522\n",
      "Test loss  1.02681337714 | accuracy  0.7454\n",
      "epoch 13\n",
      "Train loss 0.129656467155 | accuracy 0.9561\n",
      "Test loss  1.05546965599 | accuracy  0.7429\n",
      "epoch 14\n",
      "Train loss 0.103009486593 | accuracy 0.96486\n",
      "Test loss  1.10113052249 | accuracy  0.7431\n",
      "epoch 15\n",
      "Train loss 0.0821349279247 | accuracy 0.97318\n",
      "Test loss  1.08702196896 | accuracy  0.7525\n",
      "epoch 16\n",
      "Train loss 0.0738314890452 | accuracy 0.97588\n",
      "Test loss  1.21544888496 | accuracy  0.7365\n",
      "epoch 17\n",
      "Train loss 0.0631131895892 | accuracy 0.97982\n",
      "Test loss  1.1932873404 | accuracy  0.7494\n",
      "epoch 18\n",
      "Train loss 0.048470169805 | accuracy 0.985\n",
      "Test loss  1.20610188961 | accuracy  0.7568\n",
      "epoch 19\n",
      "Train loss 0.0413642635951 | accuracy 0.9881\n",
      "Test loss  1.23545114219 | accuracy  0.7515\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = preprocess(X_train_, X_test_)\n",
    "\n",
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print('epoch', epoch)\n",
    "    \n",
    "    # Training\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    perm = np.random.permutation(N_train)\n",
    "    \n",
    "    for i in range(0, N_train, batchsize):\n",
    "        x = Variable(cuda.to_gpu(X_train[perm[i: i+batchsize]]))\n",
    "        t = Variable(cuda.to_gpu(y_train[perm[i: i+batchsize]]))\n",
    "        \n",
    "        optimizer.update(model, x, t)\n",
    "        sum_loss += cuda.to_cpu(model.loss.data) * len(x.data)\n",
    "        pred_y.extend(np.argmax(cuda.to_cpu(model.y.data), axis=1)) \n",
    "    \n",
    "    loss = sum_loss / N_train\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * np.eye(10)[y_train[perm]]) / N_train\n",
    "    print('Train loss', loss, '| accuracy', accuracy)  \n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    \n",
    "    for i in range(0, N_test, batchsize):\n",
    "        x = Variable(cuda.to_gpu(X_test[i: i+batchsize]))\n",
    "        t = Variable(cuda.to_gpu(y_test[i: i+batchsize]))\n",
    "        \n",
    "        sum_loss += cuda.to_cpu(model(x, t).data) * len(x.data)\n",
    "        pred_y.extend(np.argmax(cuda.to_cpu(model.y.data), axis=1))\n",
    "\n",
    "    loss = sum_loss / N_test\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * np.eye(10)[y_test]) / N_test\n",
    "    print('Test loss ', loss, '| accuracy ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: CIFAR-10で高精度を目指そう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 普通に実装すると、CIFAR-10の識別精度は75%程度\n",
    "- これでも昔に比べたら十分凄いが、いろいろ工夫すると、80%、90%と精度を伸ばすことができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ネットワークを深くする\n",
    "先ほどの解答では畳み込み層が3層、全結合層が2層だったが、これをもっと増やす。\n",
    "- 畳み込み層の増やし方\n",
    "\n",
    "畳み込み層では、padを入れることでサイズを維持できることはすでに学んだ。  \n",
    "しかし、プーリング層では、必ずサイズが半分になる。  \n",
    "CIFAR-10の画像サイズは32\\*32なので、プーリングはせいぜい3回が限界。これは変えられない。  \n",
    "では、畳み込み層の数も3層が限界？？\n",
    "\n",
    "\n",
    "畳み込み層とプーリング層が交互に来る必要はない。  \n",
    "プーリング層とプーリング層の間には、畳み込み層が何層あっても良い！   \n",
    "\n",
    "\n",
    "そのときに、サイズの小さい畳み込み層を複数積み重ねるとよい。\n",
    "たとえば、\n",
    "\n",
    "```python\n",
    "conv1 = L.Convolution2D(3, 32, 5, pad=2),\n",
    "```\n",
    "\n",
    "としていたところを、\n",
    "```python\n",
    "conv11 = L.Convolution2D(3, 32, 3, pad=1),\n",
    "conv12 = L.Convolution2D(32, 32, 3, pad=1),\n",
    "```\n",
    "のようにする。  \n",
    "フィルタサイズ5x5の畳み込みを1回するのと、3x3を2回するのでは、やっていることはほとんど同じだが、  \n",
    "間にReLUが入ることで非線形性が増し、表現力が上がると言われている。\n",
    "\n",
    "あとは、畳み込み層のチャンネル数を増やしてみるとどうなるか？？  \n",
    "いろいろ試してみてください。\n",
    "\n",
    "- 全結合層の増やし方\n",
    "\n",
    "3層にするのが一般的。2層目は入力次元数と出力次元数を同じにすることが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dropoutを入れる\n",
    "Dropoutとは、学習時にランダムに選ばれた半分(または任意の割合)のニューロンを無視して学習を行うテクニック。  \n",
    "毎回違うニューロンが学習に回されるので、学習データに過剰に適合してしまう「過学習」を防ぐことができる。  \n",
    "テスト時は全ニューロンを用いるので、各ニューロンの出力を半分にしてやる必要がある。  \n",
    "基本的に全結合層(最終層以外)に使用する。\n",
    "\n",
    "- Chainerでの書き方\n",
    "\n",
    "call関数内で、F.dropoutで使用できる。学習時とテスト時で挙動が違うので、学習時かどうかを引数に与える必要がある。  \n",
    "無視する割合はデフォルトで0.5なので指定しなくて良い。\n",
    "```python\n",
    "def __call__(self, x, t, train):\n",
    "    # 略\n",
    "    h = F.relu(self.fc4(h))\n",
    "    h = F.dropout(h, train=train)  \n",
    "    # 略\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Normalizationを入れる\n",
    "Batch Normalization (BN)は、各層の出力をチャンネルごとに正規化(=平均0、分散1にすること)するテクニック。  \n",
    "層を多層にすると、データの値の分布が途中でどんどん変なところへ行ってしまい、学習が上手くいかない。  \n",
    "BNを入れると、それを防げる。\n",
    "\n",
    "<img src=\"figure/bn.JPG\", width=350>\n",
    "\n",
    "\n",
    "学習時はミニバッチの平均と分散を計算して正規化する。  \n",
    "テスト時は、学習時に溜めておいた平均と分散を用いて正規化する。  \n",
    "\n",
    "\n",
    "実際には、正規化するだけではなく、さらにパラメータ$\\gamma$と$\\beta$を用いてデータを拡大・平行移動している。  \n",
    "この$\\gamma$と$\\beta$が学習すべきパラメータ。チャンネル数分に対応した分だけの$\\gamma$と$\\beta$がある。\n",
    "\n",
    "基本的に畳み込み層の後に全部入れるとよい。全結合層には入れない。\n",
    "\n",
    "- Chainerでの書き方\n",
    "\n",
    "init関数内で、L.BatchNormalization(チャンネル数)で定義できる。\n",
    "```python\n",
    "super(CNN, self).__init__(\n",
    "            conv1 = L.Convolution2D(3, 32, 5, pad=2),\n",
    "            norm1 = L.BatchNormalization2D(32),\n",
    "            # 略\n",
    "        )\n",
    "```\n",
    "\n",
    "Dropoutと同様に、学習時とテスト時で挙動が違うので、call関数内で呼び出す際には引数が必要。BNでは、テスト時かどうかを表す引数が必要。  \n",
    "順番がややこしいが、conv -> norm -> reluの順。\n",
    "\n",
    "```python\n",
    "def __call__(self, x, t, train):\n",
    "    h = F.relu(self.norm(self.conv1(x), test=not train))\n",
    "    # 略\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Learning rateを調整する\n",
    "learning rate = 1回の学習で重みをどれだけ更新するか\n",
    "\n",
    "- 初期値\n",
    "\n",
    "小さすぎると最初のどうでもいい穴にハマってしまうので、最初は大きめのlearning rateで学習をしたほうがよい。  \n",
    "広い世界を冒険した方が、より低いところを見つけられる可能性が高まる。  \n",
    "ただし、あまりにも大きすぎると発散してしまうので注意。  \n",
    "いろいろ試してみてもいいが、CIFAR-10に対してはおそらく0.01がベスト。\n",
    "\n",
    "<img src=\"figure/lr1.png\", width=500>\n",
    "\n",
    "- 学習の終盤\n",
    "\n",
    "ずっと初期値のままで学習を進めていくと、今度は逆に最終的に入りたい穴に入ることができない。  \n",
    "そこで、学習が佳境に差し掛かったら、学習係数を10分の1とかにしてやる。  \n",
    "上手くいくと、その瞬間に精度がカクンと上がります！\n",
    "\n",
    "<img src=\"figure/lr2.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data augmentation\n",
    "学習データを擬似的に水増しする手法。  \n",
    "もしかしたら一番識別精度に効いてくる部分かもしれないので、是非いろいろ試してほしいです。\n",
    "\n",
    "\n",
    "よく使われるのが、画像の一部を切り取って入力するcropping、画像のスケールを変えて入力するscaling、左右反転して入力するflippingの3つ。  \n",
    "他にも、回転させたり、色を微妙に変えたりすることもある。\n",
    "\n",
    "あらかじめaugmentした画像を用意するのでは画像の枚数が増えすぎてしまうので、  \n",
    "ミニバッチごとに切り取る位置やスケールなどのパラメータを生成してaugmentするのが普通。  \n",
    "\n",
    "また、学習時だけでなく、テスト時にも同じようにいろんなパラメータでaugmentして、全部の画像を入力し、  \n",
    "得られた最終層の値の平均を取って識別するということをすると、精度がさらに上がる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
