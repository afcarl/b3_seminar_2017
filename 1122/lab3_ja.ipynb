{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# 演習 1: マルコフ決定過程 (MDP)\n",
    "\n",
    "\n",
    "## 注意\n",
    "全ての回答はこのノートブック内に書いてください．他のファイルを書いたり編集したりする必要はありません．\n",
    "\n",
    "**セル間には依存関係があるため，全てのブロックを実行してください**\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 試行ベース (表形式) Q-Learning\n",
    "\n",
    "モデルがMDPであることが必要な方策反復 (Value Iteration) や 価値反復 (Policy Iteration) は，環境がブラックボックスな物理シミュレータとして与えられ全ての場合の変遷を得られないような場合などに限界があります．\n",
    "\n",
    "このような環境でも，試行ベースの Q learning により学習を行うことができます．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この演習ではクローラロボットの制御を学びます．まずは完全にランダムな行動によりロボットがどのように動くかを見て，Gym に慣れましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from crawler_env import CrawlingRobotEnv\n",
    "\n",
    "env = CrawlingRobotEnv()\n",
    "\n",
    "print(\"この Gym 環境の観察空間と行動空間を見てみましょう．\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(\"行動空間:\", env.action_space)\n",
    "print(\"これは可能な %i 種の行動についての離散空間です．\" % env.action_space.n)\n",
    "print(\"各行動は各関節の角度を増加，減少させることに対応します．\")\n",
    "print(\"行動空間でランダムに試行をすることができます．:\", env.action_space.sample())\n",
    "print(\"2回目の試行:\", env.action_space.sample())\n",
    "print(\"3回目の試行\", env.action_space.sample())\n",
    "print(\"観察空間:\", env.observation_space, \", これは 9x13 のグリッドであることを意味します.\")\n",
    "print(\"ロボットの2つの関節の角度の離散表現です．\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CrawlingRobotEnv(\n",
    "    render=True, # turn render mode on to visualize random motion\n",
    ")\n",
    "\n",
    "# standard procedure for interfacing with a Gym environment\n",
    "cur_state = env.reset() # reset environment and get initial state\n",
    "ret = 0.\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample() # sample an action randomly\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    ret += reward\n",
    "    cur_state = next_state\n",
    "    i += 1\n",
    "    if i == 1500:\n",
    "        break # for the purpose of this visualization, let's only run for 1500 steps\n",
    "        # also note the GUI won't close automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can close the visualization GUI with the following method \n",
    "env.close_gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダム制御も時々は成功しますが，とてもうまくいくわけではないことがわかります．\n",
    "より良い戦略を得るため，ε-グリーディー探索による表形式 Q-learning を少しずつ実装して行きましょう．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# dictionary that maps from state, s, to a numpy array of Q values [Q(s, a_1), Q(s, a_2) ... Q(s, a_n)]\n",
    "#   and everything is initialized to 0.\n",
    "q_vals = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "\n",
    "print(\"状態 (0, 0) のQ値: %s\" % q_vals[(0, 0)], \"これはそれぞれの行動についてのQ値のリストになっています．\")\n",
    "print(\"つまり，状態 (1, 2) において行動 3 を行う時の Q 値 i.e. Q((1,2), 3) は，このようにアクセスできます q_vals[(1,2)][3]:\", q_vals[(1,2)][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eps_greedy(q_vals, eps, state):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        q_vals: q value tables\n",
    "        eps: epsilon\n",
    "        state: current state\n",
    "    Outputs:\n",
    "        random action with probability of eps; argmax Q(s, .) with probability of (1-eps)\n",
    "    \"\"\"\n",
    "    # you might want to use random.random() to implement random exploration\n",
    "    #   number of actions can be read off from len(q_vals[state])\n",
    "    import random\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# test 1\n",
    "dummy_q = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "test_state = (0, 0)\n",
    "dummy_q[test_state][0] = 10.\n",
    "trials = 100000\n",
    "sampled_actions = [\n",
    "    int(eps_greedy(dummy_q, 0.3, test_state))\n",
    "    for _ in range(trials)\n",
    "]\n",
    "freq = np.sum(np.array(sampled_actions) == 0) / trials\n",
    "tgt_freq = 0.3 / env.action_space.n + 0.7\n",
    "if np.isclose(freq, tgt_freq, atol=1e-2):\n",
    "    print(\"Test1 passed\")\n",
    "else:\n",
    "    print(\"Test1: Expected to select 0 with frequency %.2f but got %.2f\" % (tgt_freq, freq))\n",
    "    \n",
    "# test 2\n",
    "dummy_q = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "test_state = (0, 0)\n",
    "dummy_q[test_state][2] = 10.\n",
    "trials = 100000\n",
    "sampled_actions = [\n",
    "    int(eps_greedy(dummy_q, 0.5, test_state))\n",
    "    for _ in range(trials)\n",
    "]\n",
    "freq = np.sum(np.array(sampled_actions) == 2) / trials\n",
    "tgt_freq = 0.5 / env.action_space.n + 0.5\n",
    "if np.isclose(freq, tgt_freq, atol=1e-2):\n",
    "    print(\"Test2 passed\")\n",
    "else:\n",
    "    print(\"Test2: Expected to select 2 with frequency %.2f but got %.2f\" % (tgt_freq, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は Q learning の更新則を実装します． $s, a, s', r$ という変化を観察した時，以下のように更新されます．\n",
    "\n",
    "$$\\textrm{target}(s') = R(s,a,s') + \\gamma \\max_{a'} Q_{\\theta_k}(s',a')$$\n",
    "\n",
    "\n",
    "$$Q_{k+1}(s,a) \\leftarrow (1-\\alpha) Q_k(s,a) + \\alpha \\left[ \\textrm{target}(s') \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning_update(gamma, alpha, q_vals, cur_state, action, next_state, reward):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        gamma: discount factor\n",
    "        alpha: learning rate\n",
    "        q_vals: q value table\n",
    "        cur_state: current state\n",
    "        action: action taken in current state\n",
    "        next_state: next state results from taking `action` in `cur_state`\n",
    "        reward: reward received from this transition\n",
    "    \n",
    "    Performs in-place update of q_vals table to implement one step of Q-learning\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# testing your q_learning_update implementation\n",
    "dummy_q = q_vals.copy()\n",
    "test_state = (0, 0)\n",
    "test_next_state = (0, 1)\n",
    "dummy_q[test_state][0] = 10.\n",
    "dummy_q[test_next_state][1] = 10.\n",
    "q_learning_update(0.9, 0.1, dummy_q, test_state, 0, test_next_state, 1.1)\n",
    "tgt = 10.01\n",
    "if np.isclose(dummy_q[test_state][0], tgt,):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Q(test_state, 0) is expected to be %.2f but got %.2f\" % (tgt, dummy_q[test_state][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with the main components tested, we can put everything together to create a complete q learning agent\n",
    "\n",
    "env = CrawlingRobotEnv() \n",
    "q_vals = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "eps = 0.5\n",
    "cur_state = env.reset()\n",
    "\n",
    "def greedy_eval():\n",
    "    \"\"\"evaluate greedy policy w.r.t current q_vals\"\"\"\n",
    "    test_env = CrawlingRobotEnv(horizon=np.inf)\n",
    "    prev_state = test_env.reset()\n",
    "    ret = 0.\n",
    "    done = False\n",
    "    H = 100\n",
    "    for i in range(H):\n",
    "        action = np.argmax(q_vals[prev_state])\n",
    "        state, reward, done, info = test_env.step(action)\n",
    "        ret += reward\n",
    "        prev_state = state\n",
    "    return ret / H\n",
    "\n",
    "for itr in range(300000):\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: use eps_greedy & q_learning_update\n",
    "    \n",
    "    if itr % 50000 == 0: # evaluation\n",
    "        print(\"Itr %i # Average speed: %.2f\" % (itr, greedy_eval()))\n",
    "\n",
    "# at the end of learning your crawler should reach a speed of >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習が成功したら，ロボット制御を可視化して見ましょう！環境自体の dynamics を使うことなく，環境との相互干渉のみで学習を行なっているにも関わらず，うまく学習できていることがわかります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CrawlingRobotEnv(render=True, horizon=500)\n",
    "prev_state = env.reset()\n",
    "ret = 0.\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_vals[prev_state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    ret += reward\n",
    "    prev_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can close the visualization GUI with the following method \n",
    "env.close_gui()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
