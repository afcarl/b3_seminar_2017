{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2回 (11/9)　Multi Layer Perceptron (MLP) のライブラリ無し実装\n",
    "- もちろんnumpyは使います！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. MNISTデータの読み込み\n",
    "- MNIST: 手書き数字の大規模データセット\n",
    "    - 0〜9の10クラス\n",
    "    - 訓練データ60000枚、テストデータ10000枚\n",
    "    - 28\\*28のグレースケール画像\n",
    "- 研究室のサーバから読み込む\n",
    "\n",
    "\n",
    "- X: 画像データ\n",
    "    - X_trainは60000\\*784, X_testは10000\\*784の行列\n",
    "- y: ラベル\n",
    "    - np.eye(10)[〜]によってone-of-k表現にしておく\n",
    "- N: サンプル数\n",
    "- 一般的なDeep Learningライブラリでは変数はfloat32もしくはint32型に統一するのでここでもそうしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dir = '/data/ishimochi0/dataset/mnist/'\n",
    "\n",
    "#  訓練データ\n",
    "X_train = np.loadtxt(dir + 'train-images.txt').reshape((-1, 784)).astype(np.float32) / 255.\n",
    "y_train = np.loadtxt(dir + 'train-labels.txt').astype(np.int32)\n",
    "y_train = np.eye(10)[y_train].astype(np.int32)\n",
    "N_train = len(X_train)\n",
    "\n",
    "# テストデータ\n",
    "X_test = np.loadtxt(dir + 'test-images.txt').reshape((-1, 784)).astype(np.float32) / 255.\n",
    "y_test = np.loadtxt(dir + 'test-labels.txt').astype(np.int32)\n",
    "y_test = np.eye(10)[y_test].astype(np.int32)\n",
    "N_test = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 活性化関数とその微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid関数\n",
    "$$\n",
    "\\sigma(x) = \\frac{1} {1+e^{-x}}\n",
    "$$\n",
    "<img src=\"figure/sigmoid.png\", width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))# WRITE ME!\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return self(x) * (1 -  self(x))# WRITE ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigmoid = Sigmoid()\n",
    "\n",
    "x = np.arange(10).reshape((2, 5)) - 5\n",
    "print(x)\n",
    "\n",
    "print(sigmoid(x)) # call関数(=普通のsigmoid関数)が実行される\n",
    "print(sigmoid.deriv(x)) # 導関数が実行される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu関数\n",
    "$$\n",
    "relu(x) = max(0, x)\n",
    "$$\n",
    "<img src=\"figure/relu.png\", width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント\n",
    "- 入力xはデータ数\\*次元数の行列であることに注意\n",
    "- 「x>0」とすると、xの各成分の正負に応じたTrueとFalseからなる行列が得られる\n",
    "- Trueは1, Falseは0として振舞う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape((2, 5)) - 5\n",
    "print(x)\n",
    "\n",
    "print(x > 0)\n",
    "\n",
    "print(1 * True)\n",
    "print(1 * False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        return x * (x > 0)# WRITE ME!\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return 1 * (x > 0)# WRITE ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relu = ReLU()\n",
    "\n",
    "x = np.arange(10).reshape(2, 5) - 5\n",
    "print(x)\n",
    "\n",
    "print(relu(x))\n",
    "print(relu.deriv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax関数\n",
    "$$\n",
    "softmax(x_{k}) = \\frac{e^{x_{k}}} {\\Sigma_{i=1}^{K}{e^{x_{i}}}}\n",
    "$$\n",
    "- 多クラス識別ネットワークの出力層に必ず使用する活性化関数\n",
    "- xの各成分の対数を取り、各行の和が1になるよう正規化する\n",
    "- 各データが各クラスに属する確率を表す\n",
    "\n",
    "#### ヒント\n",
    "各行の和を取るにはnp.sum()を使うが...\n",
    "- 普通にnp.sum(a)とすると、aの全要素の和が計算されてしまう。行方向の和を取るには??\n",
    "- それだけだと、割り算のサイズが合わないと怒られてしまう。行方向の和を取ったときに、行方向の次元数が1になってしまうからである。それを回避するには??\n",
    " - Shift+tabで関数の使い方を見てみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __call__(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True) # WRITE ME! \n",
    "\n",
    "    def deriv(self, x):\n",
    "        return self(x) * (1 -  self(x))# WRITE ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax = Softmax()\n",
    "\n",
    "x = np.arange(10).reshape(2, 5) - 5\n",
    "print(x)\n",
    "\n",
    "print(softmax(x))\n",
    "print(softmax.deriv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 線形層クラス\n",
    "- 作成時の引数: 入力次元数in_dim、出力次元数out_dim、活性化関数の種類\n",
    "\n",
    "\n",
    "- 内部パラメータ: 重みW(=in_dim*out_dim), バイアスb(=out_dim), 誤差delta\n",
    " - Wの初期値は一様分布に従う乱数。範囲は上手い決め方があるが今回は-0.08〜0.08とする\n",
    " - bの初期値は0ベクトル\n",
    " - どちらもfloat32型にしておこう\n",
    " \n",
    " \n",
    "- call関数: 順伝播の計算\n",
    " - u: 入力xに重みWとバイアスbを適用した結果\n",
    " - z: uに活性化関数を適用した結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim, activation):\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim)).astype(np.float32)# WRITE ME!\n",
    "        self.b = np.zeros(out_dim).astype(np.float32)# WRITE ME!\n",
    "        self.delta = None\n",
    "        self.activation = activation()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.u = np.dot(x, self.W) + self.b# WRITE ME!\n",
    "        self.z = self.activation(self.u)# WRITE ME!\n",
    "        return self.z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLPクラス\n",
    "- 作成時の引数: 層のリストlayers=[Linear(〜), Linear(〜), ...]\n",
    "- 3層の場合の例:\n",
    "<img src=\"figure/train.png\", width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def train(self, x, t, lr):     \n",
    "        # 1. 順伝播\n",
    "        self.y = x\n",
    "        for layer in self.layers:\n",
    "            self.y = layer(self.y)# WRITE ME!\n",
    "        self.loss = np.sum(-t*np.log(self.y)) / len(x)# WRITE ME!\n",
    "        \n",
    "        # 2. 誤差逆伝播\n",
    "        # 最終層の誤差\n",
    "        delta = self.y - t# WRITE ME!\n",
    "        self.layers[-1].delta = delta\n",
    "        W = self.layers[-1].W\n",
    "        \n",
    "        # 中間層の誤差を計算\n",
    "        # 各ループ開始時に、一つ上の層の誤差と重みがそれぞれdelta、Wに格納されている\n",
    "        for layer in self.layers[-2::-1]:\n",
    "            delta = np.dot(delta, W.T) * layer.activation.deriv(layer.u)# WRITE ME!\n",
    "            layer.delta = delta\n",
    "            W = layer.W\n",
    "        \n",
    "        # 3. 各層のパラメータを更新\n",
    "        # 各ループ開始時に、一つ下の層の出力がzに格納されている\n",
    "        z = x\n",
    "        for layer in self.layers:\n",
    "            dW = np.dot(z.T, layer.delta)# WRITE ME!\n",
    "            db = np.dot(np.ones(len(z)), layer.delta)# WRITE ME!\n",
    "            layer.W -= lr * dW# WRITE ME!\n",
    "            layer.b -= lr * db# WRITE ME!\n",
    "            z = layer.z\n",
    "            \n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    def test(self, x, t):\n",
    "        # 順伝播 (train関数と全く同じでOK)\n",
    "        self.y = x\n",
    "        for layer in self.layers:\n",
    "            self.y = layer(self.y)# WRITE ME!\n",
    "        self.loss = np.sum(-t*np.log(self.y)) / len(x)# WRITE ME!\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 準備はここまで。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MLP([Linear(784, 1000, Sigmoid),\n",
    "                        Linear(1000, 1000, Sigmoid),\n",
    "                        Linear(1000, 10, Softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MNISTの学習\n",
    "- 来週chainerを使って実装するときとほっとんど同じ書き方をしました！\n",
    "- 注意点\n",
    " - 学習時は毎回順番がランダムになるようにする\n",
    " - accuracyの計算\n",
    " - print文はカンマを付けると改行されない\n",
    " - 学習に影響はないが、時々log0エラーが出てlossがnanになることがある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print('epoch %d |' % epoch,)\n",
    "    \n",
    "    # Training\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    perm = np.random.permutation(N_train)\n",
    "    \n",
    "    for i in range(0, N_train, batchsize):\n",
    "        x = X_train[perm[i:i+batchsize]]\n",
    "        t = y_train[perm[i:i+batchsize]]\n",
    "        \n",
    "        sum_loss += model.train(x, t, lr) * len(x)\n",
    "        pred_y.extend(np.argmax(model.y, axis=1))\n",
    "    \n",
    "    loss = sum_loss / N_train\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * y_train[perm]) / N_train\n",
    "    print('Train loss %.3f, accuracy %.4f |' %(loss, accuracy),)\n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    \n",
    "    for i in range(0, N_test, batchsize):\n",
    "        x = X_test[i: i+batchsize]\n",
    "        t = y_test[i: i+batchsize]\n",
    "        \n",
    "        sum_loss += model.test(x, t) * len(x)\n",
    "        pred_y.extend(np.argmax(model.y, axis=1))\n",
    "\n",
    "    loss = sum_loss / N_test\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * y_test) / N_test\n",
    "    print('Test loss %.3f, accuracy %.4f' %(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- デフォルトのネットワークでは、97%台後半程度の識別精度が出たと思います。\n",
    "- 中間層の活性化関数をReLUに変えてみたり、層数を増やしてみたりすると...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
