{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2回 (11/9)　Multi Layer Perceptron (MLP) のライブラリ無し実装\n",
    "- もちろんnumpyは使います！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. MNISTデータの読み込み\n",
    "- MNIST: 手書き数字の大規模データセット\n",
    "    - 0〜9の10クラス\n",
    "    - 訓練データ60000枚、テストデータ10000枚\n",
    "    - 28\\*28のグレースケール画像\n",
    "- 研究室のサーバから読み込む\n",
    "\n",
    "\n",
    "- X: 画像データ\n",
    "    - X_trainは60000\\*784, X_testは10000\\*784の行列\n",
    "- y: ラベル\n",
    "    - np.eye(10)[〜]によってone-of-k表現にしておく\n",
    "- N: サンプル数\n",
    "- 一般的なDeep Learningライブラリでは変数はfloat32もしくはint32型に統一するのでここでもそうしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dir = '/data/ishimochi0/dataset/mnist/'\n",
    "\n",
    "#  訓練データ\n",
    "X_train = np.loadtxt(dir + 'train-images.txt').reshape((-1, 784)).astype(np.float32) / 255.\n",
    "y_train = np.loadtxt(dir + 'train-labels.txt').astype(np.int32)\n",
    "y_train = np.eye(10)[y_train].astype(np.int32)\n",
    "N_train = len(X_train)\n",
    "\n",
    "# テストデータ\n",
    "X_test = np.loadtxt(dir + 'test-images.txt').reshape((-1, 784)).astype(np.float32) / 255.\n",
    "y_test = np.loadtxt(dir + 'test-labels.txt').astype(np.int32)\n",
    "y_test = np.eye(10)[y_test].astype(np.int32)\n",
    "N_test = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 活性化関数とその微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid関数\n",
    "$$\n",
    "\\sigma(x) = \\frac{1} {1+e^{-x}}\n",
    "$$\n",
    "<img src=\"figure/sigmoid.png\", width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))# WRITE ME!\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return self(x) * (1 -  self(x))# WRITE ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5 -4 -3 -2 -1]\n",
      " [ 0  1  2  3  4]]\n",
      "[[ 0.00669285  0.01798621  0.04742587  0.11920292  0.26894142]\n",
      " [ 0.5         0.73105858  0.88079708  0.95257413  0.98201379]]\n",
      "[[ 0.00664806  0.01766271  0.04517666  0.10499359  0.19661193]\n",
      " [ 0.25        0.19661193  0.10499359  0.04517666  0.01766271]]\n"
     ]
    }
   ],
   "source": [
    "sigmoid = Sigmoid()\n",
    "\n",
    "x = np.arange(10).reshape((2, 5)) - 5\n",
    "print x\n",
    "\n",
    "print sigmoid(x) # call関数(=普通のsigmoid関数)が実行される\n",
    "print sigmoid.deriv(x) # 導関数が実行される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu関数\n",
    "$$\n",
    "relu(x) = max(0, x)\n",
    "$$\n",
    "<img src=\"figure/relu.png\", width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント\n",
    "- 入力xはデータ数\\*次元数の行列であることに注意\n",
    "- 「x>0」とすると、xの各成分の正負に応じたTrueとFalseからなる行列が得られる\n",
    "- Trueは1, Falseは0として振舞う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5 -4 -3 -2 -1]\n",
      " [ 0  1  2  3  4]]\n",
      "[[False False False False False]\n",
      " [False  True  True  True  True]]\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(10).reshape((2, 5)) - 5\n",
    "print x\n",
    "\n",
    "print x > 0\n",
    "\n",
    "print 1 * True\n",
    "print 1 * False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        return x * (x > 0)# WRITE ME!\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return 1 * (x > 0)# WRITE ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5 -4 -3 -2 -1]\n",
      " [ 0  1  2  3  4]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 2 3 4]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "\n",
    "x = np.arange(10).reshape(2, 5) - 5\n",
    "print x\n",
    "\n",
    "print relu(x) \n",
    "print relu.deriv(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax関数\n",
    "$$\n",
    "softmax(x_{k}) = \\frac{e^{x_{k}}} {\\Sigma_{i=1}^{K}{e^{x_{i}}}}\n",
    "$$\n",
    "- 多クラス識別ネットワークの出力層に必ず使用する活性化関数\n",
    "- xの各成分の対数を取り、各行の和が1になるよう正規化する\n",
    "- 各データが各クラスに属する確率を表す\n",
    "\n",
    "#### ヒント\n",
    "各行の和を取るにはnp.sum()を使うが...\n",
    "- 普通にnp.sum(a)とすると、aの全要素の和が計算されてしまう。行方向の和を取るには??\n",
    "- それだけだと、割り算のサイズが合わないと怒られてしまう。行方向の和を取ったときに、行方向の次元数が1になってしまうからである。それを回避するには??\n",
    " - Shift+tabで関数の使い方を見てみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __call__(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True) # WRITE ME! \n",
    "\n",
    "    def deriv(self, x):\n",
    "        return self(x) * (1 -  self(x))# WRITE ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5 -4 -3 -2 -1]\n",
      " [ 0  1  2  3  4]]\n",
      "[[ 0.01165623  0.03168492  0.08612854  0.23412166  0.63640865]\n",
      " [ 0.01165623  0.03168492  0.08612854  0.23412166  0.63640865]]\n",
      "[[ 0.01152036  0.03068099  0.07871042  0.17930871  0.23139268]\n",
      " [ 0.01152036  0.03068099  0.07871042  0.17930871  0.23139268]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "\n",
    "x = np.arange(10).reshape(2, 5) - 5\n",
    "print x\n",
    "\n",
    "print softmax(x) \n",
    "print softmax.deriv(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 線形層クラス\n",
    "- 作成時の引数: 入力次元数in_dim、出力次元数out_dim、活性化関数の種類\n",
    "\n",
    "\n",
    "- 内部パラメータ: 重みW(=in_dim*out_dim), バイアスb(=out_dim), 誤差delta\n",
    " - Wの初期値は一様分布に従う乱数。範囲は上手い決め方があるが今回は-0.08〜0.08とする\n",
    " - bの初期値は0ベクトル\n",
    " - どちらもfloat32型にしておこう\n",
    " \n",
    " \n",
    "- call関数: 順伝播の計算\n",
    " - u: 入力xに重みWとバイアスbを適用した結果\n",
    " - z: uに活性化関数を適用した結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim, activation):\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim)).astype(np.float32)# WRITE ME!\n",
    "        self.b = np.zeros(out_dim).astype(np.float32)# WRITE ME!\n",
    "        self.delta = None\n",
    "        self.activation = activation()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.u = np.dot(x, self.W) + self.b# WRITE ME!\n",
    "        self.z = self.activation(self.u)# WRITE ME!\n",
    "        return self.z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLPクラス\n",
    "- 作成時の引数: 層のリストlayers=[Linear(〜), Linear(〜), ...]\n",
    "- 3層の場合の例:\n",
    "<img src=\"figure/train.png\", width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def train(self, x, t, lr):     \n",
    "        # 1. 順伝播\n",
    "        self.y = x\n",
    "        for layer in self.layers:\n",
    "            self.y = layer(self.y)# WRITE ME!\n",
    "        self.loss = np.sum(-t*np.log(self.y)) / len(x)# WRITE ME!\n",
    "        \n",
    "        # 2. 誤差逆伝播\n",
    "        # 最終層の誤差\n",
    "        delta = self.y - t# WRITE ME!\n",
    "        self.layers[-1].delta = delta\n",
    "        W = self.layers[-1].W\n",
    "        \n",
    "        # 中間層の誤差を計算\n",
    "        # 各ループ開始時に、一つ上の層の誤差と重みがそれぞれdelta、Wに格納されている\n",
    "        for layer in self.layers[-2::-1]:\n",
    "            delta = np.dot(delta, W.T) * layer.activation.deriv(layer.u)# WRITE ME!\n",
    "            layer.delta = delta\n",
    "            W = layer.W\n",
    "        \n",
    "        # 3. 各層のパラメータを更新\n",
    "        # 各ループ開始時に、一つ下の層の出力がzに格納されている\n",
    "        z = x\n",
    "        for layer in self.layers:\n",
    "            dW = np.dot(z.T, layer.delta)# WRITE ME!\n",
    "            db = np.dot(np.ones(len(z)), layer.delta)# WRITE ME!\n",
    "            layer.W -= lr * dW# WRITE ME!\n",
    "            layer.b -= lr * db# WRITE ME!\n",
    "            z = layer.z\n",
    "            \n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    def test(self, x, t):\n",
    "        # 順伝播 (train関数と全く同じでOK)\n",
    "        self.y = x\n",
    "        for layer in self.layers:\n",
    "            self.y = layer(self.y)# WRITE ME!\n",
    "        self.loss = np.sum(-t*np.log(self.y)) / len(x)# WRITE ME!\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 準備はここまで。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MLP([Linear(784, 1000, Sigmoid),\n",
    "                        Linear(1000, 1000, Sigmoid),\n",
    "                        Linear(1000, 10, Softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MNISTの学習\n",
    "- 来週chainerを使って実装するときとほっとんど同じ書き方をしました！\n",
    "- 注意点\n",
    " - 学習時は毎回順番がランダムになるようにする\n",
    " - accuracyの計算\n",
    " - print文はカンマを付けると改行されない\n",
    " - 学習に影響はないが、時々log0エラーが出てlossがnanになることがある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | Train loss 0.988, accuracy 0.6825 | Test loss 0.324, accuracy 0.9012\n",
      "epoch 1 | Train loss 0.293, accuracy 0.9121 | Test loss 0.241, accuracy 0.9268\n",
      "epoch 2 | Train loss 0.224, accuracy 0.9319 | Test loss 0.232, accuracy 0.9273\n",
      "epoch 3 | Train loss 0.186, accuracy 0.9426 | Test loss 0.160, accuracy 0.9493\n",
      "epoch 4 | Train loss 0.156, accuracy 0.9519 | Test loss 0.148, accuracy 0.9536\n",
      "epoch 5 | Train loss 0.132, accuracy 0.9597 | Test loss 0.139, accuracy 0.9564\n",
      "epoch 6 | Train loss 0.116, accuracy 0.9643 | Test loss 0.127, accuracy 0.9580\n",
      "epoch 7 | Train loss 0.103, accuracy 0.9686 | Test loss 0.106, accuracy 0.9672\n",
      "epoch 8 | Train loss 0.091, accuracy 0.9724 | Test loss 0.104, accuracy 0.9687\n",
      "epoch 9 | Train loss 0.082, accuracy 0.9752 | Test loss 0.105, accuracy 0.9667\n",
      "epoch 10 | Train loss 0.074, accuracy 0.9775 | Test loss 0.081, accuracy 0.9724\n",
      "epoch 11 | Train loss 0.067, accuracy 0.9800 | Test loss 0.088, accuracy 0.9706\n",
      "epoch 12 | Train loss 0.061, accuracy 0.9812 | Test loss 0.080, accuracy 0.9741\n",
      "epoch 13 | Train loss 0.055, accuracy 0.9829 | Test loss 0.081, accuracy 0.9749\n",
      "epoch 14 | Train loss 0.050, accuracy 0.9843 | Test loss 0.077, accuracy 0.9758\n",
      "epoch 15 | Train loss 0.046, accuracy 0.9859 | Test loss 0.073, accuracy 0.9781\n",
      "epoch 16 | Train loss 0.041, accuracy 0.9874 | Test loss 0.076, accuracy 0.9763\n",
      "epoch 17 | Train loss 0.037, accuracy 0.9889 | Test loss 0.073, accuracy 0.9769\n",
      "epoch 18 | Train loss 0.035, accuracy 0.9897 | Test loss 0.069, accuracy 0.9791\n",
      "epoch 19 | Train loss 0.031, accuracy 0.9912 | Test loss 0.070, accuracy 0.9786\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print 'epoch %d |' % epoch,\n",
    "    \n",
    "    # Training\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    perm = np.random.permutation(N_train)\n",
    "    \n",
    "    for i in xrange(0, N_train, batchsize):\n",
    "        x = X_train[perm[i:i+batchsize]]\n",
    "        t = y_train[perm[i:i+batchsize]]\n",
    "        \n",
    "        sum_loss += model.train(x, t, lr) * len(x)\n",
    "        pred_y.extend(np.argmax(model.y, axis=1))\n",
    "    \n",
    "    loss = sum_loss / N_train\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * y_train[perm]) / N_train\n",
    "    print 'Train loss %.3f, accuracy %.4f |' %(loss, accuracy), \n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    \n",
    "    for i in xrange(0, N_test, batchsize):\n",
    "        x = X_test[i: i+batchsize]\n",
    "        t = y_test[i: i+batchsize]\n",
    "        \n",
    "        sum_loss += model.test(x, t) * len(x)\n",
    "        pred_y.extend(np.argmax(model.y, axis=1))\n",
    "\n",
    "    loss = sum_loss / N_test\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * y_test) / N_test\n",
    "    print 'Test loss %.3f, accuracy %.4f' %(loss, accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- デフォルトのネットワークでは、97%台後半程度の識別精度が出たと思います。\n",
    "- 中間層の活性化関数をReLUに変えてみたり、層数を増やしてみたりすると...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
