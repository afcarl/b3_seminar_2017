{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2回 (11/9)　Multi Layer Perceptron (MLP) のライブラリ無し実装\n",
    "- もちろんnumpyは使います！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. MNISTデータの読み込み\n",
    "- MNIST: 手書き数字の大規模データセット\n",
    "    - 0〜9の10クラス\n",
    "    - 訓練データ60000枚、テストデータ10000枚\n",
    "    - 28\\*28のグレースケール画像\n",
    "- 研究室のサーバから読み込む\n",
    "\n",
    "\n",
    "- X: 画像データ\n",
    "    - X_trainは60000\\*784, X_testは10000\\*784の行列\n",
    "- y: ラベル\n",
    "    - np.eye(10)[〜]によってone-of-k表現にしておく\n",
    "- N: サンプル数\n",
    "- 一般的なDeep Learningライブラリでは変数はfloat32もしくはint32型に統一するのでここでもそうしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dir = '/data/ishimochi0/dataset/mnist/'\n",
    "\n",
    "#  訓練データ\n",
    "X_train = np.loadtxt(dir + 'train-images.txt').reshape((-1, 784)).astype(np.float32) / 255.\n",
    "y_train = np.loadtxt(dir + 'train-labels.txt').astype(np.int32)\n",
    "y_train = np.eye(10)[y_train].astype(np.int32)\n",
    "N_train = len(X_train)\n",
    "\n",
    "# テストデータ\n",
    "X_test = np.loadtxt(dir + 'test-images.txt').reshape((-1, 784)).astype(np.float32) / 255.\n",
    "y_test = np.loadtxt(dir + 'test-labels.txt').astype(np.int32)\n",
    "y_test = np.eye(10)[y_test].astype(np.int32)\n",
    "N_test = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 活性化関数とその微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid関数\n",
    "$$\n",
    "\\sigma(x) = \\frac{1} {1+e^{-x}}\n",
    "$$\n",
    "<img src=\"figure/sigmoid.png\", width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))# WRITE ME!\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return self(x) * (1 - self(x))# WRITE ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid = Sigmoid()\n",
    "\n",
    "x = np.arange(10).reshape((2, 5)) - 5\n",
    "print(x)\n",
    "\n",
    "print(sigmoid(x)) # call関数(=普通のsigmoid関数)が実行される\n",
    "print(sigmoid.deriv(x)) # 導関数が実行される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu関数\n",
    "$$\n",
    "relu(x) = max(0, x)\n",
    "$$\n",
    "<img src=\"figure/relu.png\", width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント\n",
    "- 入力xはデータ数\\*次元数の行列であることに注意\n",
    "- 「x>0」とすると、xの各成分の正負に応じたTrueとFalseからなる行列が得られる\n",
    "- Trueは1, Falseは0として振舞う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape((2, 5)) - 5\n",
    "print(x)\n",
    "\n",
    "print(x > 0)\n",
    "\n",
    "print(1 * True)\n",
    "print(1 * False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        return x * (x > 0)# WRITE ME!\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return (x > 0).astype(np.float32)# WRITE ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relu = ReLU()\n",
    "\n",
    "x = np.arange(10).reshape(2, 5) - 5\n",
    "print(x)\n",
    "\n",
    "print(relu(x))\n",
    "print(relu.deriv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax関数\n",
    "$$\n",
    "softmax(x_{k}) = \\frac{e^{x_{k}}} {\\Sigma_{i=1}^{K}{e^{x_{i}}}}\n",
    "$$\n",
    "- 多クラス識別ネットワークの出力層に必ず使用する活性化関数\n",
    "- xの各成分の対数を取り、各行の和が1になるよう正規化する\n",
    "- 各データが各クラスに属する確率を表す\n",
    "\n",
    "#### ヒント\n",
    "各行の和を取るにはnp.sum()を使うが...\n",
    "- 普通にnp.sum(a)とすると、aの全要素の和が計算されてしまう。行方向の和を取るには??\n",
    "- それだけだと、割り算のサイズが合わないと怒られてしまう。行方向の和を取ったときに、行方向の次元数が1になってしまうからである。それを回避するには??\n",
    " - Shift+tabで関数の使い方を見てみよう\n",
    "- これをそのまま実装すると、$x$の値が大きすぎる時に計算がうまくいかない時がある。（値のオーバーフロー）\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    softmax(x_{k}) \n",
    "    &= \\frac{e^{x_{k}}} {\\Sigma_{i=1}^{K}{e^{x_{i}}}} \\\\\\\n",
    "    &= \\frac{Ce^{x_{k}}} {C\\Sigma_{i=1}^{K}{e^{x_{i}}}} \\\\\\\n",
    "    &= \\frac{\\exp(x_{k} - C')} {\\Sigma_{i=1}^{K}{\\exp(x_{i} - C')}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "であるので、 $C' = \\max\\{ x_{1}, \\cdots ,x_{K} \\}$ とすることで回避できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __call__(self, x):\n",
    "        c = np.max(x, axis=1)[:, np.newaxis]\n",
    "        result = np.exp(x - c) / np.sum(np.exp(x - c), axis=1)[:, np.newaxis]\n",
    "        return result # WRITE ME! \n",
    "\n",
    "    def deriv(self, x):\n",
    "        return self(x) * (1 - self(x))# WRITE ME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape(2, 5)\n",
    "print(x.shape)\n",
    "print(x[:, :, np.newaxis].shape)\n",
    "print(x.reshape(2, 5, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 答え合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax = Softmax()\n",
    "\n",
    "x = np.arange(10).reshape(2, 5) - 5\n",
    "print(x)\n",
    "\n",
    "print(softmax(x))\n",
    "print(softmax.deriv(x))\n",
    "x = np.array([[1010, 1000, 990],\n",
    "                        [2020, 2000, 1980]])\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 線形層クラス\n",
    "- 作成時の引数: 入力次元数in_dim、出力次元数out_dim、活性化関数の種類\n",
    "\n",
    "\n",
    "- 内部パラメータ: 重みW(=in_dim\\*out_dim), バイアスb(=out_dim), 誤差delta\n",
    " - Wの初期値は一様分布に従う乱数。範囲は上手い決め方があるが今回は-0.08〜0.08とする\n",
    " - bの初期値は0ベクトル\n",
    " - どちらもfloat32型にしておこう\n",
    " \n",
    " \n",
    "- call関数: 順伝播の計算\n",
    " - u: 入力xに重みWとバイアスbを適用した結果\n",
    " - z: uに活性化関数を適用した結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(low=-0.08, high=0.08, size=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim, activation):\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim)).astype(np.float32) # WRITE ME!\n",
    "        self.b = np.zeros(out_dim).astype(np.float32)# WRITE ME!\n",
    "        self.delta = None\n",
    "        self.activation = activation()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.u = x @ self.W + self.b# WRITE ME!\n",
    "        #self.u = np.dot(x, self.W) + self.b \n",
    "        self.z = self.activation(self.u)# WRITE ME!\n",
    "        return self.z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLPクラス\n",
    "- 作成時の引数: 層のリストlayers=[Linear(〜), Linear(〜), ...]\n",
    "- 3層の場合の例:\n",
    "<img src=\"figure/train.png\", width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 3, 5, 6, 9])\n",
    "print(x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(x[-2::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def train(self, x, t, lr):     \n",
    "        # 1. 順伝播\n",
    "        self.y = x\n",
    "        for layer in self.layers:\n",
    "            self.y = layer(self.y)# WRITE ME!\n",
    "        \n",
    "        # t.shape = (N, K)\n",
    "        # y.shape = (N, K)\n",
    "        self.loss = -np.sum(t * np.log(self.y + 1e-7)) / len(x)# WRITE ME!\n",
    "        \n",
    "        # 2. 誤差逆伝播\n",
    "        # 最終層の誤差\n",
    "        delta = self.y - t# WRITE ME!\n",
    "        self.layers[-1].delta = delta\n",
    "        W = self.layers[-1].W\n",
    "        \n",
    "        # 中間層の誤差を計算\n",
    "        # 各ループ開始時に、一つ上の層の誤差と重みがそれぞれdelta、Wに格納されている\n",
    "        for layer in self.layers[-2::-1]:\n",
    "            delta = delta @ W.T * layer.activation.deriv(layer.u)# WRITE ME!\n",
    "            layer.delta = delta\n",
    "            W = layer.W\n",
    "        \n",
    "        # 3. 各層のパラメータを更新\n",
    "        # 各ループ開始時に、一つ下の層の出力がzに格納されている\n",
    "        z = x\n",
    "        for layer in self.layers:\n",
    "            dW = z.T @ layer.delta# WRITE ME!\n",
    "            db = np.ones(len(z)) @ layer.delta# WRITE ME!\n",
    "            layer.W -= lr * dW# WRITE ME!\n",
    "            layer.b -= lr * db# WRITE ME!\n",
    "            z = layer.z\n",
    "            \n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    def test(self, x, t):\n",
    "        # 順伝播 (train関数と全く同じでOK)\n",
    "        self.y = x\n",
    "        for layer in self.layers:\n",
    "            self.y = layer(self.y)# WRITE ME!\n",
    "        self.loss = -np.sum(t * np.log(self.y + 1e-7)) / len(x)# WRITE ME!\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 準備はここまで。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLP([Linear(784, 1000, Sigmoid),\n",
    "                        Linear(1000, 1000, Sigmoid),\n",
    "                        Linear(1000, 10, Softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MNISTの学習\n",
    "- 来週chainerを使って実装するときとほっとんど同じ書き方をしました！\n",
    "- 注意点\n",
    " - 学習時は毎回順番がランダムになるようにする\n",
    " - accuracyの計算\n",
    " - print文はカンマを付けると改行されない\n",
    " - 学習に影響はないが、時々log0エラーが出てlossがnanになることがある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "lr = 0.01\n",
    "\n",
    "losses = {'train':[], 'test': []}\n",
    "accuracies = {'train':[], 'test':[]}\n",
    "for epoch in range(n_epoch):\n",
    "    print('epoch %d |' % epoch,)\n",
    "    \n",
    "    # Training\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    perm = np.random.permutation(N_train)\n",
    "    \n",
    "    for i in range(0, N_train, batchsize):\n",
    "        x = X_train[perm[i:i+batchsize]]\n",
    "        t = y_train[perm[i:i+batchsize]]\n",
    "        \n",
    "        sum_loss += model.train(x, t, lr) * len(x)\n",
    "        pred_y.extend(np.argmax(model.y, axis=1))\n",
    "    \n",
    "    loss = sum_loss / N_train\n",
    "    losses['train'].append(loss)\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * y_train[perm]) / N_train\n",
    "    accuracies['train'].append(accuracy)\n",
    "    print('Train loss %.3f, accuracy %.4f |' %(loss, accuracy),)\n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    \n",
    "    for i in range(0, N_test, batchsize):\n",
    "        x = X_test[i: i+batchsize]\n",
    "        t = y_test[i: i+batchsize]\n",
    "        \n",
    "        sum_loss += model.test(x, t) * len(x)\n",
    "        pred_y.extend(np.argmax(model.y, axis=1))\n",
    "\n",
    "    loss = sum_loss / N_test\n",
    "    losses['test'].append(loss)\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * y_test) / N_test\n",
    "    accuracies['test'].append(accuracy)\n",
    "    print('Test loss %.3f, accuracy %.4f' %(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- デフォルトのネットワークでは、97.5%程度の識別精度が出たと思います。\n",
    "- 中間層の活性化関数をReLUに変えてみたり、層数を増やしてみたりすると...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss, accuracyの可視化\n",
    "- 学習がうまくいっているか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, n_epoch + 1), losses['train'], 'blue', label='Train Loss')\n",
    "plt.plot(np.arange(1, n_epoch + 1), losses['test'], 'orange', label='Test Loss')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(1, n_epoch + 1), accuracies['train'], \\\n",
    "         'blue', label='Train Accuracy')\n",
    "plt.plot(np.arange(1, n_epoch + 1), accuracies['test'], \\\n",
    "         'orange', label='Test Accuracy')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracies\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
