{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第3回 (11/16) 宿題: CIFAR-10で高精度を目指そう\n",
    "- 次回(11/30)までにいろいろ試して、高精度を目指してみて下さい！\n",
    "- ベースラインとして、75%出るCNNを書いておきました。自由に書き換えてしまって構いません。\n",
    "- 床爪のベストスコアは94.7%です。越えてみてください！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Variable, Chain, optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import cPickle as pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "train = [unpickle('/data/ishimochi0/dataset/cifar-10-batches-py/data_batch_%d' %i) for i in range(1,6)]\n",
    "X_train_ = np.concatenate([d['data'] for d in train]).reshape((-1, 3, 32, 32)).astype('float32') / 255.\n",
    "y_train = np.concatenate([d['labels'] for d in train]).astype('int32')\n",
    "N_train = len(X_train_)\n",
    "\n",
    "test = unpickle('/data/ishimochi0/dataset/cifar-10-batches-py/test_batch')\n",
    "X_test_ = test['data'].reshape((-1, 3, 32, 32)).astype('float32') / 255.\n",
    "y_test = np.array(test['labels'], dtype='int32')\n",
    "N_test = len(X_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(X_train_, X_test_):\n",
    "    X_mean = np.mean(X_train_, axis=(0, 2, 3), keepdims=True)\n",
    "    X_train = X_train_ - X_mean\n",
    "    X_test = X_test_ - X_mean\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(Chain):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__(\n",
    "            conv1 = L.Convolution2D(3, 32, 5, pad=2),\n",
    "            conv2 = L.Convolution2D(32, 64, 5, pad=2),\n",
    "            conv3 = L.Convolution2D(64, 128, 5, pad=2),\n",
    "            fc4 = L.Linear(16*128, 128),\n",
    "            fc5 = L.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "    \n",
    "        h = F.relu(self.fc4(h))\n",
    "        self.y = self.fc5(h)\n",
    "        self.loss = F.softmax_cross_entropy(self.y, t)\n",
    "        \n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義とoptimizerの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CNN() \n",
    "model.to_gpu()\n",
    "optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(0.0005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess(X_train_, X_test_)\n",
    "\n",
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print 'epoch %d |' % epoch,\n",
    "    \n",
    "    # Training\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    perm = np.random.permutation(N_train)\n",
    "    \n",
    "    for i in xrange(0, N_train, batchsize):\n",
    "        x = Variable(cuda.to_gpu(X_train[perm[i: i+batchsize]]))\n",
    "        t = Variable(cuda.to_gpu(y_train[perm[i: i+batchsize]]))\n",
    "        \n",
    "        optimizer.update(model, x, t)\n",
    "        sum_loss += cuda.to_cpu(model.loss.data) * len(x.data)\n",
    "        pred_y.extend(np.argmax(cuda.to_cpu(model.y.data), axis=1)) \n",
    "    \n",
    "    loss = sum_loss / N_train\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * np.eye(10)[y_train[perm]]) / N_train\n",
    "    print 'Train loss %.3f, accuracy %.4f |' %(loss, accuracy), \n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    sum_loss = 0\n",
    "    pred_y = []\n",
    "    \n",
    "    for i in xrange(0, N_test, batchsize):\n",
    "        x = Variable(cuda.to_gpu(X_test[i: i+batchsize]))\n",
    "        t = Variable(cuda.to_gpu(y_test[i: i+batchsize]))\n",
    "        \n",
    "        sum_loss += cuda.to_cpu(model(x, t).data) * len(x.data)\n",
    "        pred_y.extend(np.argmax(cuda.to_cpu(model.y.data), axis=1))\n",
    "\n",
    "    loss = sum_loss / N_test\n",
    "    accuracy = np.sum(np.eye(10)[pred_y] * np.eye(10)[y_test]) / N_test\n",
    "    print 'Test loss %.3f, accuracy %.4f' %(loss, accuracy) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
